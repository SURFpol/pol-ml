{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vereiste imports\n",
    "\n",
    "import os, nltk, re, pdftotext, docx2txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "from stop_words import get_stop_words\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie voor het verwerken van een map met data\n",
    "# Input: Filepath (String), Nummer van de categorie, Naam van de categorie (String)\n",
    "# Output: Pandas DataFrame\n",
    "\n",
    "def process_docs(directory, category_num, category_desc):\n",
    "    data1 = []\n",
    "    # Vaak voorkomende irrelevante woorden\n",
    "    filtered_words = ['creativ','common','kennisnet','copyright','http','licentie']\n",
    "    for filename in os.listdir(directory):\n",
    "        #print(filename)\n",
    "        #print(\"\")\n",
    "        if filename.endswith((\".pdf\", \".PDF\")):\n",
    "            with open(directory+\"/\"+filename, \"rb\") as f:\n",
    "                pdf = pdftotext.PDF(f)\n",
    "            text=(\"\\n\\n\".join(pdf))\n",
    "            # Wanneer document ingescand is, kan text extracted worden via tesseract\n",
    "            if text != \"\":\n",
    "                text = text\n",
    "            else:\n",
    "                try:\n",
    "                    text = textract.process((directory+\"/\"+filename), method='tesseract',\n",
    "                                            language='nld', errors='ignore')\n",
    "                    text = text.decode(\"utf-8\")\n",
    "                except Exception:\n",
    "                    print(filename, \"textract.process & text.decode\")\n",
    "                    pass\n",
    "            # Tokenizen van de tekst\n",
    "            tokens = word_tokenize(text)\n",
    "            # Controleer op en verwijder NL-talige stopwoorden\n",
    "            stop_words = get_stop_words('dutch')\n",
    "            keywords = [word for word in tokens if not word in stop_words]\n",
    "            # Stem alle woorden, alle vervoegingen van een werkwoord worden teruggebracht tot de stam\n",
    "            from nltk.stem.snowball import DutchStemmer\n",
    "            DutchStemmer = DutchStemmer()\n",
    "            keywords = [DutchStemmer.stem(word) for word in keywords]\n",
    "            # Verwijder alle niet-alfabetische karakters\n",
    "            keywords2 = [re.sub(r\"\\s*[^A-Za-z]+\\s*\", '', word) for word in keywords]\n",
    "            # Verwijder dubbele spaties tussen woorden\n",
    "            keywords3 = [re.sub(' +',' ',word) for word in keywords2]\n",
    "            # Verwijder woorden die kleiner zijn dan 3 letters\n",
    "            keywords3 = [re.sub(r'\\b\\w{1,3}\\b', '', word) for word in keywords3]\n",
    "            # Vervang alle hoofdletters door kleine letters\n",
    "            keywords3 = [word.lower() for word in keywords3]\n",
    "            # Verwijder extreem lange woorden, groter dan 20 letters\n",
    "            keywords3 = [word for word in keywords3 if len(word) <= 20]\n",
    "            # Verwijder veel voorkomende, irrelevante woorden\n",
    "            keywords3 = [word for word in keywords3 if word.strip()]\n",
    "            keywords3 = [word for word in keywords3 if not word in filtered_words]\n",
    "            keywords3 = ' '.join(keywords3)\n",
    "            # Check of document minder dan 100 woorden bevat, in dat geval overslaan\n",
    "            if len(keywords3) <= 100:\n",
    "                print(filename, '<= 100 words')\n",
    "                print(len(keywords3))\n",
    "                print(\"\")\n",
    "                continue\n",
    "            else:\n",
    "                data1.extend([[category_num,category_desc,keywords3]])\n",
    "            continue\n",
    "        elif filename.endswith((\".docx\", \".DOCX\")):\n",
    "            f = str(directory+\"/\"+filename)\n",
    "            text = docx2txt.process(f)\n",
    "            # Tokenizen van de tekst\n",
    "            tokens = word_tokenize(text)\n",
    "            # Controleer op en verwijder NL-talige stopwoorden\n",
    "            stop_words = get_stop_words('dutch')\n",
    "            keywords = [word for word in tokens if not word in stop_words]\n",
    "            # Stem alle woorden, alle vervoegingen van een werkwoord worden teruggebracht tot de stam\n",
    "            from nltk.stem.snowball import DutchStemmer\n",
    "            DutchStemmer = DutchStemmer()\n",
    "            keywords = [DutchStemmer.stem(word) for word in keywords]\n",
    "            # Verwijder alle niet-alfabetische karakters\n",
    "            keywords2 = [re.sub(r\"\\s*[^A-Za-z]+\\s*\", '', word) for word in keywords]\n",
    "            # Verwijder dubbele spaties tussen woorden\n",
    "            keywords3 = [re.sub(' +',' ',word) for word in keywords2]\n",
    "            # Verwijder woorden die kleiner zijn dan 3 letters\n",
    "            keywords3 = [re.sub(r'\\b\\w{1,3}\\b', '', word) for word in keywords3]\n",
    "            # Vervang alle hoofdletters door kleine letters\n",
    "            keywords3 = [word.lower() for word in keywords3]\n",
    "            # Verwijder extreem lange woorden, groter dan 20 letters\n",
    "            keywords3 = [word for word in keywords3 if len(word) <= 20]\n",
    "            # Verwijder veel voorkomende, irrelevante woorden\n",
    "            keywords3 = [word for word in keywords3 if word.strip()]\n",
    "            keywords3 = [word for word in keywords3 if not word in filtered_words]\n",
    "            keywords3 = ' '.join(keywords3)\n",
    "            # Check of document minder dan 100 woorden bevat, in dat geval overslaan\n",
    "            if len(keywords3) <= 100:\n",
    "                print(filename, '<= 100 words')\n",
    "                print(len(keywords3))\n",
    "                print(\"\")\n",
    "                continue\n",
    "            else:\n",
    "                data1.extend([[category_num,category_desc,keywords3]])\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "    df = pd.DataFrame(data1, columns = ['CategoryID', 'Category', 'Text'])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereer per map/categorie een apart dataframe.\n",
    "\n",
    "df = process_docs(\"/path/to/category/les\", 1, \"Les\")\n",
    "df2 = process_docs(\"/path/to/category/opdracht\", 2, \"Opdracht\")\n",
    "#df3 = process_docs(\"/path/to/category/toets\", 3, \"Toets\")\n",
    "df4 = process_docs(\"/path/to/category/referentie\", 4, \"Referentie\")\n",
    "\n",
    "# Voeg de dataframes samen tot een enkel dataframe.\n",
    "\n",
    "df = df.append(df2)\n",
    "#df = df.append(df3)\n",
    "df = df.append(df4)\n",
    "\n",
    "# Verwijder eventuele lege entries uit de dataframe, reset de nummering van de index \n",
    "# en sla het dataframe op als .pkl-bestand.\n",
    "\n",
    "df = df[df.Text != '']\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_pickle(\"texts_pdfs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genereer dictionaries voor plots.\n",
    "\n",
    "category_id_df = df[['Category', 'CategoryID']].drop_duplicates().sort_values('CategoryID')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['CategoryID', 'Category']].values)\n",
    "\n",
    "# Check de onderlinge verdeling binnen de dataset.\n",
    "\n",
    "%matplotlib inline\n",
    "df.groupby('Category').Text.count().plot.bar(ylim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roep de TF-IDF vectorizer aan.\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=6, norm='l2', encoding='utf-8', ngram_range=(1, 2), \n",
    "                        stop_words=(get_stop_words('dutch')), max_df=0.7, max_features=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pas TF-IDF toe op de data.\n",
    "\n",
    "features = tfidf.fit_transform(df.Text).toarray()\n",
    "labels = df.CategoryID\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot welke woorden per categorie het meest aan elkaar zijn gecorreleerd.\n",
    "\n",
    "N = 3\n",
    "for category, category_id in sorted(category_to_id.items()):\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}':\".format(category))\n",
    "    print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:])))\n",
    "    print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de TF-IDF vector per document.\n",
    "\n",
    "SAMPLE_SIZE = int(len(features))\n",
    "np.random.seed(2)\n",
    "indices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)\n",
    "projected_features = TSNE(n_components=2, random_state=0).fit_transform(features[indices])\n",
    "colors = ['pink', 'green', 'midnightblue', 'orange', 'darkgrey']\n",
    "for category, category_id in sorted(category_to_id.items()):\n",
    "    points = projected_features[(labels[indices] == category_id).values]\n",
    "    plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[category_id], label=category)\n",
    "plt.title(\"tf-idf feature vector for each document, projected on 2 dimensions.\",\n",
    "          fontdict=dict(fontsize=12)) \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roep het Logistic Regression-model aan.\n",
    "\n",
    "model2 = LogisticRegression(solver='lbfgs',random_state=42, multi_class='auto')\n",
    "\n",
    "# Splits de dataset in een train- en test-set.\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit het model op de data.\n",
    "\n",
    "model2.fit(X_train, y_train)\n",
    "print(\"Accuracy:\",model2.score(X_test, y_test))\n",
    "\n",
    "# Laat het model een voorspelling doen op de test-set.\n",
    "\n",
    "y_pred_proba = model2.predict_proba(X_test)\n",
    "y_pred = model2.predict(X_test)\n",
    "\n",
    "# Plot een confusion matrix van de resultaten op de test-set.\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.Category.values, yticklabels=category_id_df.Category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valideer het getrainde model evt. op een map met aparte bestanden.\n",
    "\n",
    "directory = \"/path/to/folder\"\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "        #print(filename)\n",
    "        #print(\"\")\n",
    "        if filename.endswith((\".pdf\", \".PDF\")):\n",
    "            with open(directory+\"/\"+filename, \"rb\") as f:\n",
    "                extracted_text = []\n",
    "                pdf = pdftotext.PDF(f)\n",
    "                text=(\"\\n\\n\".join(pdf))\n",
    "                # Wanneer document ingescand is, kan text extracted worden via tesseract\n",
    "                if text != \"\":\n",
    "                    text = text\n",
    "                else:\n",
    "                    try:\n",
    "                        text = textract.process((directory+\"/\"+filename), method='tesseract',\n",
    "                                                language='nld', errors='ignore')\n",
    "                        text = text.decode(\"utf-8\")\n",
    "                    except Exception:\n",
    "                        print(filename, \"textract.process & text.decode\")\n",
    "                        pass\n",
    "                # Tokenizen van de tekst\n",
    "                tokens = word_tokenize(text)\n",
    "                # Controleer op en verwijder NL-talige stopwoorden\n",
    "                stop_words = get_stop_words('dutch')\n",
    "                keywords = [word for word in tokens if not word in stop_words]\n",
    "                # Stem alle woorden, alle vervoegingen van een werkwoord worden teruggebracht tot de stam\n",
    "                from nltk.stem.snowball import DutchStemmer\n",
    "                DutchStemmer = DutchStemmer()\n",
    "                keywords = [DutchStemmer.stem(word) for word in keywords]\n",
    "                # Verwijder alle niet-alfabetische karakters\n",
    "                keywords2 = [re.sub(r\"\\s*[^A-Za-z]+\\s*\", '', word) for word in keywords]\n",
    "                # Verwijder dubbele spaties tussen woorden\n",
    "                keywords3 = [re.sub(' +',' ',word) for word in keywords2]\n",
    "                # Verwijder woorden die kleiner zijn dan 3 letters\n",
    "                keywords3 = [re.sub(r'\\b\\w{1,3}\\b', '', word) for word in keywords3]\n",
    "                # Vervang alle hoofdletters door kleine letters\n",
    "                keywords3 = [word.lower() for word in keywords3]\n",
    "                # Verwijder extreem lange woorden, groter dan 20 letters\n",
    "                keywords3 = [word for word in keywords3 if len(word) <= 20]\n",
    "                # Verwijder veel voorkomende, irrelevante woorden\n",
    "                keywords3 = [word for word in keywords3 if word.strip()]\n",
    "                keywords3 = ' '.join(keywords3)\n",
    "                extracted_text.append(keywords3)\n",
    "                text_features = tfidf.transform(extracted_text)\n",
    "                predictions = model2.predict(text_features)\n",
    "                if \"les\" in filename and predictions == 1:\n",
    "                    #print(filename)\n",
    "                    #print(\"Correct\")\n",
    "                    #print(\"\")\n",
    "                    correct += 1\n",
    "                elif \"opdracht\" in filename and predictions == 2:\n",
    "                    #print(filename)\n",
    "                    #print(\"Correct\")\n",
    "                    #print(\"\")\n",
    "                    correct += 1\n",
    "                elif \"referentie\" in filename and predictions == 4:\n",
    "                    #print(filename)\n",
    "                    #print(\"Correct\")\n",
    "                    #print(\"\")\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    print(filename)\n",
    "                    print(\"Incorrect, guess was\", predictions)\n",
    "                    incorrect += 1\n",
    "                    print(\"\")\n",
    "        elif filename.endswith((\".docx\", \".DOCX\")):\n",
    "            extracted_text = []\n",
    "            f = str(directory+\"/\"+filename)\n",
    "            text = docx2txt.process(f)\n",
    "            # Tokenizen van de tekst\n",
    "            tokens = word_tokenize(text)\n",
    "            # Controleer op en verwijder NL-talige stopwoorden\n",
    "            stop_words = get_stop_words('dutch')\n",
    "            keywords = [word for word in tokens if not word in stop_words]\n",
    "            # Stem alle woorden, alle vervoegingen van een werkwoord worden teruggebracht tot de stam\n",
    "            from nltk.stem.snowball import DutchStemmer\n",
    "            DutchStemmer = DutchStemmer()\n",
    "            keywords = [DutchStemmer.stem(word) for word in keywords]\n",
    "            # Verwijder alle niet-alfabetische karakters\n",
    "            keywords2 = [re.sub(r\"\\s*[^A-Za-z]+\\s*\", '', word) for word in keywords]\n",
    "            # Verwijder dubbele spaties tussen woorden\n",
    "            keywords3 = [re.sub(' +',' ',word) for word in keywords2]\n",
    "            # Verwijder woorden die kleiner zijn dan 3 letters\n",
    "            keywords3 = [re.sub(r'\\b\\w{1,3}\\b', '', word) for word in keywords3]\n",
    "            # Vervang alle hoofdletters door kleine letters\n",
    "            keywords3 = [word.lower() for word in keywords3]\n",
    "            # Verwijder extreem lange woorden, groter dan 20 letters\n",
    "            keywords3 = [word for word in keywords3 if len(word) <= 20]\n",
    "            # Verwijder veel voorkomende, irrelevante woorden\n",
    "            keywords3 = [word for word in keywords3 if word.strip()]\n",
    "            keywords3 = ' '.join(keywords3)\n",
    "            extracted_text.append(keywords3)\n",
    "            text_features = tfidf.transform(extracted_text)\n",
    "            predictions = model2.predict(text_features)\n",
    "            if \"les\" in filename and predictions == 1:\n",
    "                #print(filename)\n",
    "                #print(\"Correct\")\n",
    "                #print(\"\")\n",
    "                correct += 1\n",
    "            elif \"opdracht\" in filename and predictions == 2:\n",
    "                #print(filename)\n",
    "                #print(\"Correct\")\n",
    "                #print(\"\")\n",
    "                correct += 1\n",
    "            elif \"referentie\" in filename and predictions == 4:\n",
    "                #print(filename)\n",
    "                #print(\"Correct\")\n",
    "                #print(\"\")\n",
    "                correct += 1\n",
    "            else:\n",
    "                print(filename)\n",
    "                print(\"Incorrect, guess was\", predictions)\n",
    "                incorrect += 1\n",
    "                print(\"\")\n",
    "        else:\n",
    "            continue\n",
    "                    \n",
    "print(\"Correct:\", correct)\n",
    "print(\"Incorrect:\", incorrect)\n",
    "\n",
    "# 1 Les\n",
    "# 2 Opdracht\n",
    "# 3 Toets\n",
    "# 4 Referentie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de ROC Curve. De classes en n_classes moeten worden aangepast \n",
    "# wanneer er meer categorieen (toets) aan de dataset worden toegevoegd\n",
    "\n",
    "y = label_binarize(labels, classes=[1, 2, 4])\n",
    "n_classes = 3\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = features.shape\n",
    "X = features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve per class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
