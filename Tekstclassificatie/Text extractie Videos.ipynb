{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vereiste imports\n",
    "\n",
    "import textract, os, PyPDF2, nltk, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interp\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import svm, datasets\n",
    "from itertools import cycle\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwijzing naar CSV-bestand\n",
    "\n",
    "file = 'Video-Texts-NL.csv'\n",
    "\n",
    "# Lees het CSV-bestand in\n",
    "\n",
    "df = pd.read_csv(file,sep=';')\n",
    "category_id_df = df[['Category', 'CategoryID']].drop_duplicates().sort_values('CategoryID')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['CategoryID', 'Category']].values)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwerkt de tekst per video\n",
    "\n",
    "text_list = df[\"Text\"].values\n",
    "new_list = []\n",
    "for text in text_list:\n",
    "    # Tokenizen van de tekst\n",
    "    tokens = word_tokenize(text)\n",
    "    # Controleer op en verwijder NL-talige stopwoorden\n",
    "    stop_words = get_stop_words('dutch')\n",
    "    keywords = [word for word in tokens if not word in stop_words]\n",
    "    # Stem alle woorden, alle vervoegingen van een werkwoord worden teruggebracht tot de stam\n",
    "    from nltk.stem.snowball import DutchStemmer\n",
    "    DutchStemmer = DutchStemmer()\n",
    "    keywords = [DutchStemmer.stem(word) for word in keywords]\n",
    "    # Verwijder alle niet-alfabetische karakters\n",
    "    keywords2 = [re.sub(r\"\\s*[^A-Za-z]+\\s*\", '', word) for word in keywords]\n",
    "    # Verwijder dubbele spaties tussen woorden\n",
    "    keywords3 = [re.sub(' +',' ',word) for word in keywords2]\n",
    "    # Vervang alle hoofdletters door kleine letters\n",
    "    keywords3 = [word.lower() for word in keywords3]\n",
    "    # Verwijder extreem lange woorden, groter dan 20 letters\n",
    "    keywords3 = [word for word in keywords3 if len(word) <= 20]\n",
    "    # Verwijder veel voorkomende, irrelevante woorden\n",
    "    keywords3 = [word for word in keywords3 if word.strip()]\n",
    "    keywords3 = ' '.join(keywords3)\n",
    "    new_list.append(keywords3)\n",
    "df.drop(columns=['Text'])\n",
    "df['Text'] = new_list\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequentie per categorie\n",
    "\n",
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roep de TF-IDF vectorizer aan.\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2))\n",
    "\n",
    "# Pas TF-IDF toe op de data.\n",
    "\n",
    "features = tfidf.fit_transform(df.Text).toarray()\n",
    "labels = df.CategoryID\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot welke woorden per categorie het meest aan elkaar zijn gecorreleerd.\n",
    "\n",
    "N = 5\n",
    "for category, category_id in sorted(category_to_id.items()):\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}':\".format(category))\n",
    "    print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:])))\n",
    "    print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de TF-IDF vector per document.\n",
    "\n",
    "SAMPLE_SIZE = int(len(features))\n",
    "np.random.seed(250694)\n",
    "indices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)\n",
    "projected_features = TSNE(n_components=2, random_state=0).fit_transform(features[indices])\n",
    "colors = ['pink', 'green', 'midnightblue', 'orange', 'darkgrey', 'yellow', 'red', 'black']\n",
    "for category, category_id in sorted(category_to_id.items()):\n",
    "    points = projected_features[(labels[indices] == category_id).values]\n",
    "    plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[category_id], label=category)\n",
    "plt.title(\"tf-idf feature vector for each category, projected on 2 dimensions.\",\n",
    "          fontdict=dict(fontsize=12))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roep het Logistic Regression-model aan.\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs',random_state=42, multi_class='auto')\n",
    "\n",
    "# Splits de dataset in een train- en test-set.\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.99, random_state=42)\n",
    "\n",
    "# Fit het model op de data.\n",
    "\n",
    "model.fit(X_test, y_test)\n",
    "print(\"Accuracy:\",model.score(X_test, y_test))\n",
    "\n",
    "# Laat het model een voorspelling doen op de test-set.\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot een confusion matrix van de resultaten op de test-set.\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.Category.values, yticklabels=category_id_df.Category.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welke woorden dragen het meeste bij aan de toekenning aan een bepaalde categorie.\n",
    "\n",
    "model.fit(features, labels)\n",
    "\n",
    "N = 5\n",
    "for category, category_id in sorted(category_to_id.items()):\n",
    "    indices = np.argsort(model.coef_[(category_id-1)])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n",
    "    bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n",
    "    print(\"# '{}':\".format(category))\n",
    "    print(\"  . Top unigrams:\\n       . {}\".format('\\n       . '.join(unigrams)))\n",
    "    print(\"  . Top bigrams:\\n       . {}\".format('\\n       . '.join(bigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de ROC Curve. De classes en n_classes moeten worden aangepast \n",
    "# wanneer er meer categorieen aan de dataset worden toegevoegd\n",
    "\n",
    "y = label_binarize(labels, classes=[1, 2, 3, 4, 5, 6, 7])\n",
    "n_classes = 7\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = features.shape\n",
    "X = features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n",
    "                                                    random_state=2)\n",
    "\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'gray', 'purple'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,# lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')#, lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
